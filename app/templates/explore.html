{% extends "base.html" %}

{% block app_content %}
    <h1>Fundamental concepts</h1>
    <p>A <i>Boolean algebra</i> is a mathematical system $(\beta, +, \cdot)$ consisting of a nonempty
    set $\beta$ and two binary operations $+$ and $\cdot$ defined on $\beta$ such that (1) seach of the operations $+$
    and $\cdot$ is commutative; (2) each operation is distibutive over the other; (3) there exist distinct identity
    element $0$ and $1$ relative to operations $+$ and $\cdot$ respectively, that is, $a+0=a$, $a*1=a$ for all
    $a\in\beta$; (4) for each element $a\in\beta$, there exists an element $a^C\in\beta$, called <i>complement</i> of
    $a$, such that $a+a^C=1$, $a\cdot a^C=0$.</p>
    <p>We will primarily work with the two element Boolean algebra. We shall use $\beta_0$ to denote the
    set $\{0,1\}$ with three operations $+$, $\cdot$, $^C$ defined as follows: $0+0=0\cdot 1=1 \cdot 0 = 0\cdot 0=0$,
    $1+ 0=0+1=1+1=1\cdot 1=1$, $0^C=1$, and $1^C=0$.</p>
    <h3>Boolean vectors</h3>
    <p><b>Definition 1.1.</b> Let $V_n$ denote the set of all $n$-tuples $(a_1, a_2, ..., a_n)$ over
    $\beta_0$. An element of $V_n$ is called a <i>Boolean vector</i> of dimension $n$. The system $V_n$ together with
    the operation of component-wise addition is called the <i>Boolean vector space of dimension $n$</i>.</p>
    <p><b>Definition 1.2.</b> Let $V^n=\{v^T: v \in V_n\}$, where $v^T$ we mean <i>column vector</i>
    $$\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$$</p>
    <p><b>Definition 1.3.</b> Define the <i>complement</i> $v^C$ of $v$ to be the vector such that
    $v_i^C=1$ if and only if $v_i=0$, where $v_i$ denote the $i$the component of $v$.</p>
    <p><b>Definition 1.4.</b> Let $e_i$ be the $n$-tuple with $1$ as $i$th coordinate, $0$ otherwise.
    Further, we define $e^i=e_i^T$.</p>
    <p><b>Definition 1.5.</b> A $subspace$ of $V_n$ is a subset containing the zero vector and closed
    under addition of vectors. The $span$ of a set $W$ of vectors, denoted $\lt W\gt$, is the intersection of all
    subspaces containing $W$.</p>
    <p><b>Definition 1.6.</b> Let $W\subset V_n$. A vector $v\in V_n$ is said to be <i>dependent</i> on
    $W$ if and only if $v\in\lt W\gt$. A set $W$ is said to be <i>independent</i> if ond only if for all $v\in W$, $v$
    is not dependent on $W\setminus\{v\}$, where $\setminus$ denotes the set-theoretic difference. If $W$ is not
    independent we say that it is dependent.</p>
    <p><b>Definition 1.7.</b> Let $u, v \in V_n$, the we say that $u\le v$ if and only if $v_i=1$ for
    each $i$ such that $u_i=1$. We say that $u\lt v$ if $u \le v$ and $u \ne v$.</p>
    <p><b>Definition 1.8.</b> Let $W \subset V_n$. A subset $B$ of $W$ is called a <i>basis</i> of $W$
    if and only if $W = \lt B \gt$, and $B$ is an independent set.</p>
    <p><b>Theorem 1.1</b> Let $W$ be a subspace of $V_n$. Then there exists one subset $B$ of $V_n$ such
    that $B$ is a basis of $W$.</p>
    <p><b>Proof.</b> Let $B$ be the set of all vectors of $W$ that are not sums of vectors of $W$
    smaller then themselves. Then $B$ is an independent set. Suppose $B$ generates a proper subspace of $W$. The let
    $v$ be a minimal vector of $W \setminus \lt B \gt$. Then $v$ is expressible as a sum of smaller vectors since it is
    not in $B$. But these smaller vectors must be in $\lt B \gt$ since $v$ was minimal. Thus $v$ belongs in the
    subspace generated by $B$. This is a contradiction. Thus $B$ generates $W$ and is a basis. By independence, $B$
    must be contained in every basis. Let $B'$ be another basis and let $u$ be a minimal element of $B'\setminus B$.
    Then by the reasoning above $u$ is dependent. This contradiction shows $B'=B$. This proves the theorem. &#9647;</p>
    <h3>Boolean Matrices</h3>
    <p><b>Definition 2.1.</b> By a <i>Boolean matrix</i> of size $m \times n$ is meant an $m \times n$
    matrix over $\beta_0$. Let $B_{mn}$ denote the set of all $m \times n$ such matrices. If $m=n$, we write $B_n$.
    Elements of $B_{mn}$ are often called <i>relation matrices</i>, <i>Boolean relation matrices</i>, <i>binary
    relation matrices</i>,
    <i>binary Boolean matrices</i>, <i>(0, 1)-Boolean matrices</i>, and <i>(0,1)-matrices</i>.</p>
    <p><b>Definition 2.2.</b> Let $A = (a_{ij}) \in B_{mn}$. Then the element $a_{ij}$ is called the
    <i>$(i,j)$-entry</i> of $A$. The $(i,j)$-entry of $A$ is sometimes designated by $A_{ij}$. The $i$th <i>row</i> of
    $A$ is the sequence $a_{i1}, a_{i2}, ..., a_{in}$, and $j$th <i>column</i> of $A$ is the sequence $a_{1j}, a_{2j},
    ... a_{mj}$. Let $A_{i*}$ ($A_{*i}$) denote the $i$th row (column) of $A$.</p>
    <p><b>Definition 2.3.</b> The $n \times m$ <i>zero matrix $0$</i> is the matrix all of whose entries
    are zero. The $n \times n$ <i>identity matrix $I$</i> is the matrix $(\delta_{ij})$ such then $\delta_{ij}=1$ if
    $i=j$ and $\delta_{ij}=0$ if $i \ne j$. The $n \times m$ <i>universal matrix $J$</i> is the matrix all of whose
    entries are $1$.</p>
    <p><b>Definition 2.4.</b> We use the notation $A^2$ to designate the product $AA$, $A^3=AA^2$, and
    in general $A^k=AA^{k-1}$ for any positive integer $k$. The matrix $A^k$ is called the $k$th <i>power</i> for
    obvious reasons. The motations $a^{(k)}_{ij}$ and $A^{(k)}_{ij}$ means $(i,j)$-entry and $(i,j)$-block of $A^k$.
    The notation $(A_{ij})^k$ means the $k$th power of the $(i,j)$-block of $A$.</p>
    <p><b>Definition 2.5.</b> A <i>binary relation</i> on a set $X$ is a subset of $X \times X$. The
    <i>composition</i> of two binary relations $\rho_1$, $\rho_2$ is the raltion $\gamma$ such that $(x,y) \in \gamma$
    if and only if for some $z$ both $(x,z) \in \rho_1$ and $(z,y) \in \rho_2$.</p>
    <p><b>Definition 2.6.</b> The <i>adjacency matrix $A_G$</i> of a directed graph (digraph) $G$ is the
    matrix $A_G=(a_{ij})$ such that $a_{ij}=1$ if there is an arc from vertex $V_i$ to vertex $v_j$ and $a_{ij}=0$
    otherwise. Dually, a digraph $G$ determines and is determined by the Boolean matrix $A_G$.</p>
    <p><b>Definition 2.7.</b> A square matrix is called a <i>permutation matrix</i> if every row and
    every column contains exactly one $1$. Let $P_n$ denote the set of all $n \times n$ such matrices.</p>
    <p><b>Definition 2.8.</b> The <i>transpose</i> of a Boolean matrix is obtained by rewriting its rows
    and columns. The transpose of $A$ will be denoted by $A^T$.</p>
    <p><b>Definition 2.9.</b> A matrix is said to be a <i>partial permutation matrix</i> if every row
    and every column of it contains at most one $1$.</p>
    <p><b>Definition 2.10.</b> Let $A, B \in B_{mn}$. By $B \le A$ we mean if $b_{ij}=1$ then $a_{ij}=1$
    for every $i$ and $j$.</p>
    <p><b>Definition 2.11.</b> The <i>row space</i> of a matrix $A$ is the span of the set of all rows
    of $A$. Likewise one has a <i>column space</i>. Let $R(A)$ ($C(A)$) denote the row (column) space of $A$.</p>
    <p><b>Definition 2.12.</b> By a <i>partial order relation</i> on a set $X$ we mean a reflexive,
    antisymmetric and transitive relation on $X$. A set $X$ together with a specific partial order relation $P$ in $X$
    is called a partially ordered set (poset). A <i>linear order</i> (also called <i>total order</i>) is a partial
    order relation $P$ such that for all $x,y$ $(x,y) \in P$ or $(y,x) \in P$.</p>
    <p><b>Definition 2.13.</b> A <i>lattice</i> is a partially ordered set in which every pair of
    elements has a least upper bound (join) and greatest lower bound (meet). The operations join and meet are denoted
    by $\lor$ and $\land$ respectively.</p>
    <p><b>Definition 2.14.</b> A lattice is said to be <i>distributive</i> if and only if $A \lor (B
    \land C) = (A \lor B) \land (A \lor C)$ for all $A, B, C$. This is equivalent to the dual condition $A \land (B
    \lor C)=(A \land B) \lor (A \land C)$.</p>
    <p><b>Proposition 2.1.</b> If $A \in B_{mn}$, then $R(A)$ ($C(A)$) is a subspace of $V_n$
    ($V^m$).</p>
    <p><b>Proof.</b> This relation holds for matrices over any semiring, and the proof is the same as in
    the case of matrices over $\mathbb{R}$. &#9647;</p>
    <p><b>Proposition 2.2.</b> Let $A \in B_{mk}$, $B \in B_{kn}$, then $R(AB) \subseteq R(B)$ and
    $C(AB) \subseteq C(A)$.</p>
    <p><b>Proof.</b> The proof follows immediately from the fact that the rows of $AB$ are sums of the
    rows of $B$, etc. &#9647;</p>
    <p><b>Theorem 2.3.</b> If $A \in B_{mn}$, then $\lvert C(A) \rvert = \lvert R(A) \rvert$.</p>
    <p><b>Proof.</b> We shall construct a bijection between $C(A)$ and $R(A)$. Let $v \in C(A)$, then
    there exists a unique set $\underline{s} \subset \underline{m} \subseteq \underline{n}$ such that $$v =
    \sum_\underline{s} e^i$$ Let $\underline{s}' = \underline{m} \setminus \underline{s}$ (we will use $'$ to denote
    set complements in this proof). Consider the map $f: C(A) \rightarrow R(A)$ given by $$f(v) = \sum_{\underline{s}'}
    A_{i*}$$ where $v \in C(A)$. Clearly $f$ is well-defined. We claim that the following statements are true.</p>
    <p>(1) $f$ is injective: suppose we have $v,w \in C(A)$, $v \neq w$, and $$v = \sum_\underline{s}
    e^i $$ while $$w = \sum_\underline{t} e^i$$ where $\underline{t} \subset \underline{m}$ and that $f(v) = f(w)$,
    i.e., $$\sum_{s'} A_{i*} = \sum_{t'} A_{i*}$$ Since $v \neq w$, we may assume that there exist a $p \in
    \underline{t} \setminus \underline{s}$. But since $w \in C(A)$ there exists a $k \in \underline{n}$ such that
    $a_{pk}=1$ and $A_{*k} \leq w$. Since $p \in \underline{s}'$, we must have $(f(v))_k=1$, which implies that there
    exists a $q \in \underline{t}'$ such that $a_{qk}=1$. But since $A_{*k} \leq w$, this implies that $e^q \leq w$,
    which is impossible since $q \in t'$. Thus $f(v) \neq f(w)$.</p>
    <p>(2) Since there exists an injection from the row space into the column space, we are through as
    it follows that $f$ is surjective. &#9647;</p>
    <p><b>Corollary 2.4.</b> Let $A$ and $f$ be as in Theorem 2.3, and let $v,w \in C(A)$. Then $v \leq
    w$ if and only if $f(v) \geq f(w)$.</p>
    <p><b>Proof.</b> <b>Necessity.</b> Clear, in that $\underline{s} \subset \underline{t}$ if and only
    if $\underline{s}' \supset \underline{t}'$. <b>Sufficiency. </b>This follows from the same proof as that given for
    injectivity of $f$. Assume $w \nleq v$ but $f(w) \geq f(v)$, and follow exactly the same procedure. &#9647;</p>
    <p><b>Proposition 2.5.</b> Let $A_1, A_2, ..., A_k \in B_n$ and let $B = A_1 A_2 ... A_k$. Then
    $\lvert C(B)\rvert = \lvert R(B) \rvert \leq \lvert R(A_i) \rvert = \lvert R(A_I) \rvert = \lvert C(A_i) \rvert$
    for all $i$.</p>
    <p><b>Proof.</b> For any $M, N$ we have $\lvert R(MN) \rvert \leq \lvert R(N) \rvert$ and $\lvert
    R(MN) \rvert = \lvert C(MN) \rvert \leq \lvert C(M) \rvert = \lvert R(M) \rvert$, by Proposition 2.2. The present
    proposition follows from this by induction. &#9647;</p>
    <p><b>Definition 2.15.</b> Let $A \in B_{mn}$. By $B_r(A)$ we mean the unique basis of $R(A)$, and
    we call it the <i>row basis</i> of $A$. Similarly, by $B_c(A)$ we mean the unique basis of $C(A)$, which we call
    the <i>column basis</i> of $A$. The cardinality of $B_r(A)$ ($B_c(A)$) is called the <i>row (column) rank</i> of
    $A$ and is denoted by $\rho_r(A)$ ($\rho_c(A)$).</p>
    <h3>Green's Relations</h3>
    <p><b>Definition 3.1.</b> A <i>right (left) ideal</i> in a semigroup $S$ is a subset $X$ such that
    $XS \subseteq X$ ($SX \subseteq X$), and the <i>(two sided) ideal</i> of $S$ generated by $X$ is $SXS \cup XS \cup
    SX$. <i> Principal ideals</i>, <i>principal left</i> and <i>right ideals</i> are defined in a similar way.</p>
    <p><b>Definition 3.2.</b> Two elements of a semigroup $S$ are said to be
    <i>$\mathcal{L}$-equivalent</i> if they generate the same pricipal left ideal of $S$.
    <i>$\mathcal{R}$-equivalence</i> is defined dually. The join of the equivalence relations $\mathcal{L}$ and
    $\mathcal{R}$ is denoted by $\mathcal{D}$ and their intersection by $\mathcal{H}$. Two elements are said to be
    <i>$\mathcal{J}$-equivalent</i> if they generate the same two-sided pricipal ideal. These five relations are known
    as <i>Green's relations</i>.</p>
    <p><b>Definition 3.3.</b> The <i>weight</i> of a vector $v$, denoted by $w(v)$, is the number of
    nonzero elements of $v$. The weight of $v$ is sometimes called the rank of the vector $v$.</p>
    <p><b>Lemma 3.1.</b> Two matrices $A, B$ are $\mathcal{L}$ ($\mathcal{R}$)-equivalent if and only if
    they have the same row (column) space.</p>
    <p><b>Proof.</b> Suppose $XA=B$ and $YB=A$. Then $R(B) \subseteq R(A)$ and $R(A) \subseteq R(B)$ so
    $R(A) = R(B)$. Suppose $R(B) \subseteq R(A)$. Then by looking at each row of $B$ we can find an $X$ such that
    $XA=B$. Likewise we can find a $Y$ such that $YB=A$. &#9647;</p>
    <p><b>Lemma 3.2.</b> Let $U$ be any subspace of $V_n$ and $f$ a homomorphism of commutative
    semigroups from $U$ into $V_n$ such that $f(0)=0$. Then there exists a matrix $A$ such that for all $v \in V_n$,
    $vA = f(v)$.</p>
    <p><b>Proof.</b> Let $S(i)=\{v \in U; v_i = 1 \}$. Then define $A_{i*}$ to be $inf \{ f(v): v \in
    S(i)\}$. We will show $vA=f(v)$, for all $v \in U$. Suppose $(vA)_j=1$. Then for some $k$, $v_k=1$ and $a_{kj}=1$.
    Thus for all $w \in S(k)$, $(f(w))_j=1$. Since $v \in S(k)$, $(f(v))_j=1$. This proves that $vA \le f(v)$.</p>
    <p>Suppose that $(vA)_j=0$. Then for all $k$ such that $v_k=1$, we have $a_{kj}=0$. Thus for all $k$
    such that $v_k=1$ we have a vector $x(k) \in S(k)$ such that $f(x(k))_j=0$. But $(\sum x(k))_k$ is $1$ for each $k$
    such that $v_k=1$. Thus $\sum x(k) \ge v$. Thus $\sum f(x(k)) = f(\sum x(k)) \ge f(v)$. Since $f(x(k))_j=0$ for
    each $k$, $f(v)_j = 0$. This proves $vA \ge f(v)$. &#9647;</p>
    <p><b>Theorem 3.3.</b> Two matrices in $B_n$ belong in the same $\mathcal{D}$-class if and only if
    their row spaces are isomporphic as lattices.</p>
    <p><b>Proof.</b> The row space of matrix is the same as its image space on row vectors. And two
    such spaces are isomorphic as lattices if and only if they are isomorphic as commutative semigroups.</p>
    <p>Suppose $A \mathcal{D} B$. Let $C$ be such that $A \mathcal{L} C$ and $C \mathcal{R} B$. Then
    $A$, $C$ have identical image spaces on row vectors. Let $X$, $Y$ be such that $CX=B$ and $BY=C$. Then we have maps
    $f: V_n C \rightarrow V_n B$ and $g: V_n B \rightarrow V_n C$ given by multiplying on the right by $X$ and $Y$. We
    have $fg$ and $gf$ are the identity. Thus the image spaces of $B$ adn $C$ are isomorphic. Thus if $A \mathcal{D}
    B$, $R(A) \simeq R(B)$.</p>
    <p>Suppose $R(A) \simeq R(B)$. Let $h$ be an isomorphism from $V_n A$ to $V_n B$. By Lemma 3.2 we
    have matrices matrices $X$, $Y$ such that $vX=h(v)$ for $v \in V_n A$ and $vY = h^{-1}(v)$ for $v \in V_n B$. Thus
    $XY$ is the identity on $V_n A$ and $YX$ is the identity on $V_n B$. Then $A=AXY$ and $V_nAX=V_n B$. Thus $A
    \mathcal{R} AX$ and $AX \mathcal{L} B$. So $A \mathcal{D} B$. This proves the theorem. &#9647;</p>
    <p><b>Definition 3.4.</b> Let $S$ be a semigroup, and let $a \in S$. We define: $L_a=\{b \in S: a
    \mathcal{L} b\}$, $R_a = \{b \in S: a \mathcal{R} b\}$, $H_a = \{b \in S: a \mathcal{H} b\}$, $D_a = \{b \in S: a
    \mathcal{D} b\}$, $J_a = \{b \in S: a \mathcal{J} b\}$.</p>
    <p><b>Theorem 3.4.</b> Let $A \in B_n$. Then the elements of $H_a$ are in one-to-one correspondence
    with the lattice automorphisms of $R(A)$.</p>
    <p><b>Proof.</b> Let $\alpha$ be an automorphism from $R(A)$ to $R(A)$. Then $A \alpha$ gives a
    linear map from $V_n$ to $V_n$ sending $0$ to $0$, i.e., a matrix $B$. The matrices $A$, $B$, both have image space
    $R(A)$, so they are $\mathcal{L}$-equivalent. By Lemman 3.2, there exist matrices $X$, $Y$ so that on $R(A)$ the
    equations $X=\alpha$ and $Y=\alpha^{-1}$ hold. Then for any vector $v$ it is true that $v A X = v A \alpha = vB$
    and $v B Y = v B \alpha^{-1} = v A \alpha \alpha^{-1} = v A$. So $AX=B$ and $BY=A$. Thus $A \mathcal{H} B$. This
    defines a function from the automorphisms of $R(A)$ into the $\mathcal{H}$-class of $A$. Two different
    automorphisms will give rise to different maps $A \alpha$ and so to different matrices $B$. Thus the function is
    one-to-one. Let $B$ be any matrix in the $\mathcal{H}$-class of $A$. Then $R(A)=R(B)$ and there exist matrices $X$
    and $Y$ such that $AX=B$ and $BY=A$. This implices that $X$ and $Y$ map $R(A)$ to itself and that $X$ gives an
    automorphism of $R(A)$. This proves the function is onto, and completes the proof of the theorem. &#9647;</p>
    <p><b>Lemma 3.5.</b> The number of permutations of $n$ objects with repetitions allowed wich may be
    formed from $p$ objects of which $k$ have been singled out to appear in every one of these permutations is </p>
    <p class="center">$$\sum_{i=0}^k (-1)^i \begin{pmatrix}k \\ i \end{pmatrix} (p-i)^n$$</p>
    <p><b>Proof.</b> We will prove the lemma by using generating functions for permutations although the
    lemma can be proved directly by analyzing which kinds of permutations are permissible under the rule stated in the
    lemma.</p>
    <p>The generating function for the situation as described in the above will be $$(1+ t + {t^2 \over
    2!} + ...)^{p-k} = (e^t-1)^k (e^t)^{p-k} \sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} e^{(k-i) t}
    e^{(p-k) t} = \sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} e^{(p-i) t}$$ Thus this equation can be
    written as $$\sum_{n=0}^\infty \sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} (p-i)^n {t^n \over n!}$$
    Once we have the generating function, the answer to our problem is the coefficient of $t^n/n!$, which turns out to
    be $$\sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} (p-i)^n$$ This completes the proof. &#9647;</p>
    <p><b>Corollary 3.6.</b> For any integer $n$, $$n! = \sum_{i=0}^n (-1)^i \begin{pmatrix} n \\ i
    \end{pmatrix} (n-i)^n.$$</p>
    <p><b>Proof.</b> The right hand side of the equality written above is the number of permutations of
    $n$ objects of which $n$ have been singled out to appear of all permutations of $n$ objects without repetitions.
    &#9647;</p>
    <p><b>Theorem 3.7.</b> If $A \in B_n$, $\rho_r(A)=s$, $\rho_c(A)=t$, $\lvert H_A \rvert = h$, and
    $\lvert R(A) \rvert = \lvert C(A) \rvert \rvert = p$, then </p>
    <p>(1) $\vert L_A \rvert = \sum_{i=0}^s (-1)^i \begin{pmatrix} s \\ i \end{pmatrix} (p-i)^n$,
    $\lvert R_A \rvert = \sum_{i=0}^t (-1)^i \begin{pmatrix} t \\ i \end{pmatrix} (p-i)^n$ (2) the number of
    $\mathcal{L}$-classes in $D_A$ is $h^{-1} \lvert R_A \rvert$ and similarly the number of $\mathcal{R}$-classes in
    $D_A$ is $h^{-1} \lvert L_A \rvert$. The number of $\mathcal{H}$-classes equals the number of $\mathcal{L}$-classes
    multiplied by the number of $\mathcal{R}$-classes or $h^{-2} \lvert L_A \rvert \lvert R_A \rvert$ (3) $\lvert D_A
    \rvert = h$ (number of $\mathcal{L}$-classes) (number of $\mathcal{R}$-classes $= h^{-1} \lvert L_A \rvert \lvert
    R_A \rvert$</p>
    <p><b>Proof.</b> Follows directly from structure of Green's equivalence class and Lemma 3.5.
    &#9647;</p>
    <p><b>Corollary 3.8.</b> Let $A \in B_n$ and $\rho_r(A) = \rho_c(A)$. Then $\lvert L_A \rvert =
    \lvert R_A \rvert$ and the number of $\mathcal{L}$-classes in $D_A$ equals the number of $\mathcal{R}$-classes in
    $D_A$.</p>
    <p><b>Theorem 3.9.</b> For a matrix $A$ let $A_0$ be the matrix obtained from $A$ by replacing all
    dependent rows and columns of $A$, and all but one copy of each independent row and column of $A$, by zero. Then
    $A \mathcal{D} A_0$ so $H_A = \lvert H_{A_0} \rvert$. And $H_{A_0} = \{P A_0: P A_0 = A_0 Q$ for some permutation
    matrices $P, Q\}$.</p>
    <p><b>Proof.</b> There is a natural isomorphism between the row sapce of $A$ and that of $A_0$, so
    $A$ and $A_0$ are $\mathcal{D}$-equivalent. Thus $\lvert H_A \rvert = \lvert H_{A_0} \rvert$. The indicated set of
    matrices are both $\mathcal{R}$ and $\mathcal{L}$-equivalent to $A_0$ so they lie in $H_{A_0}$.</p>
    <p>Conversely let $X \in H_{A_0}$. Then $X$ has the same row space and the same column space as
    $A_0$. Suppose $X_{i*} \neq 0$ but $(A_0)_{i*}=0$. Then some column of $X$ has its $i$-entry nonzero. But no column
    of $A$ has $i$-entry nonzero. This is a contradiction.</p>
    <p>Thus $X$ can have no more nonzero rows than $A_0$. But $X$ must contain all the nonzero rows of
    $A_0$, since the nonzero rows are distinct basis vectors. Thus the rows of $X$ are permutations of the rows of
    $A_0$. Thus $X = P A_0$ for some permutation matrix $P$. Likewise $X = A_0 Q$ for some permutation matrix $Q$. This
    proves the theorem. &#9647;</p>
    <p><b>Corollary 3.10.</b> If $A$ has row or column rnak $r$ then $\lvert H_A \rvert$ divides
    $r!$</p>
    <p><b>Theorem 3.11.</b> Let $A, B \in B_n$. (1) If $A \mathcal{L} B$, then $\lvert C(A) \rvert =
    \lvert C(B) \rvert$. (2) If $A \mathcal{R} B$, then $\lvert R(A) \rvert = \lvert R(B) \rvert$. (3) If $A
    \mathcal{D} B$, then $\lvert R(A) \rvert = \lvert R(B) \rvert = \lvert C(B) \rvert = \lvert C(B) \rvert = \lvert
    C(A) \rvert$.</p>
    <p><b>Proof.</b> Obvious from Theorem 2.3 and Lemma 3.1 &#9647;</p>
    <p><b>Devinition 3.5.</b> A family $\underline{m}$ of subsets of $\underline{n}$ is an <i>additive
    space</i> if $\emptyset \in \underline{m}$ and $\underline{s} \cup \underline{t} \in \underline{m}$ where
    $\underline{s}, \underline{t} \in \underline{m}$. Two such families are isomorphic if and only if they are
    isomorphic as semigroups udner addition.</p>
    <p><b>Definition 3.6.</b> A lattice is of <i>type $(n,m)$</i> if it occurs as some subspace of $V_n$
    which is generated (except for $0$) under union by $m$ elements. (This is equivalent to saying the lattice has at
    most $m$ generators toher than $0$ under union and the lattice has $n$ generatiors other then the highest element
    under intersection).</p>
    <p><b>Lemma 3.12.</b> Let $n$, $m$ tend to infinity in such a way that $${\log n \over m}
    \rightarrow 0, {\log m \over n} \rightarrow 0$$ Then the proportion of $m \times n$ matrices which have both row
    rank $m$ and column rank $n$ tends to $1$.</p>
    <p><b>Proof.</b> Let $r(i, j)$ denote the number of matrices with $A_{i*} \ge A_{j*}$ and $c(i, j)$ the number with
    $A_{*i} \ge A_{*j}$. Then for fixed $i \ne j$ we have $${r(i,j) \over k}=({3 \over 4})^m, {c(i, j) \over k} = ({3
    \over 4})^m$$ where $k = 2 ^ {nm}$. Thus the number of matrices having no row greater than or equal to any other,
    and no column greater than or equal to any other is at least $$(1-(n^2-n) ({3 \over 4})^m - (m^2 - m) ({3 \over
    4})^n) 2^{nm}$$ All these matrices have row rank $m$ and column rank $n$. Under the given hypotheses this number
    divided by $2^{mn}$ will tend to $1$. This completes the proof. &#9647;</p>
    <p><b>Lemma 3.13.</b> If $P$ or $Q$ have no more than $k$ cycles the number of solutions $X$ of $PXQ = X$ is no more
    than $2^{kn}$ or $2^{km}$, respectively.</p>
    <p><b>Proof.</b> Let $P$ have no more than $k$ cycles. CHoose one row from each cycle, and specify it. This can be
    done in $2^{kn}$ ways, and these rows determine the rest. Similarly for $Q$. &#9647;</p>
    <p><b>Lemma 3.14.</b> If a permutation $P$ has at least $k$ cycles, it will fix at least $m-2(m-k)$ numbers from
    $\underline{m}$.</p>
    <p><b>Proof.</b> If $i$ numbers are fixed. $i + 2 (k-i) \le m$. &#9647;</p>
    <p><b>Lemma 3.15.</b> Let a permutation group $G$ act on a set $S$ of letters. If for any element $g$ of $G$, $g$
    fixes at least at $\lvert S \rvert - a$ letters, with $a > 0$, then there is a set of $\lvert S \rvert - 2a + 1$
    letters fixed by every element of $G$.</p>
    <p><b>Proof.</b> The aciton of $G$ on $S$ gives a linear representation $R$ of $G$ by permutation matrices. Let
    $o_1$, $o_2$, $...$, $o_k$, $o_{k+1}$, $...$, $o_{k+t}$ be the $G$-orbits contained in $S$, where $o_1$, $o_2$,
    $...$, $o_k$ contain only one element each, and the rest contain more than one element. Corresponding to this orbit
    decomposition we have a direct sum decomposition $R = R_1 \oplus R_2 \oplus ... R_K \oplus R_{k+1}
    \oplus ... \oplus R_{k+t}$. A theorem in group representation theory states that $$ \sum_{g \in G}
    \DeclareMathOperator{\Tr}{Tr}\Tr (g) = (k+t) \lvert G \rvert $$ But for any $g \in G$, $\Tr(g) \ge \lvert S \rvert
    - a$ and, assuming $a \ge 0$, $\Tr (\theta) > \lvert S \rvert - a$, where $\Tr (g)$ denotes the trace of $g$ and
    $\theta$ denotes the identity element of $G$. Thus $\lvert S \rvert - a < k + t$. Yest $\vert S \rvert \ge k + 2t$.
    Thus $$\lvert S \rvert - a < k + {\lvert S \rvert - k \over 2}$$ which yields the desired inequality on $k$.
    &#9647;</p>
    <p>From this lemma we derive an asymptotic bound on the number of $\mathcal{D}$-classes. Namely for two matrices
    of row rank $n$, if they are $\mathcal{L}$-equivalent they have the same row basis, but the row basis consists of
    all rows. Thus the rows of one matix are a permutation of the other matrix. Thus $A \mathcal{L} B$ if and only if
    $A=PB$ for a permutation matrix $B$. Likewise $A \mathcal{R} B$ if and only if $A=BQ$. It follows that $A
    \mathcal{D} B$ if and only if $A=PBQ$. Thus these $\mathcal{D}$-classes have at most $n!m!$ members. Thus the
    number of $\mathcal{D}$-classes of these row and column rank $n$ element is at least $${2^{nm}-o(2^{nm}) \over
    n!m!} = {2^{nm} \over n!m! } (1-O(1))$$ This gives the lower bound. To prove the upper bound, we will prove that
    the number of $D$-classes containing a matrix $X$ such that $PXQ=X$ for nonidentity permutation matrices $P, Q$
    is $$o({2^{nm} \over n!m!})$$ This means most $D$-classes have at least $n!m!$ members. Thus the number of
    $\mathcal{D}$-classes cannot exceed $${2^{nm} \over n!m!}(1-o(1))$$</p>
    <p><b>Theorem 1.4.6.</b> Let $n$, $m$ tend to infinity in such a way that $n/m$ tends to a nonzero constant. Then
    the number of $\mathcal{D}$-classes of $m \times n$ Boolean matrices is asymptotically equal to $${2^{nm} \over
    n!m!}$$</p>
    <p><b>Proof.</b> By Lemma 1.3.12 and the consideration after its proof we need only prove this formula gives an
    asymptotic upper bound.</p>
    <p>Let $k=\sup\{\lim n/m, \lim m/n\}$. Case 1. $\mathcal{D}$-classes containing some $X$ such that $PXQ=X$ for some
    $P$, $Q$ such that $P$ has no more than $m-(4k+1)\log m$ cycles. (All logarithms are base 2). For fixed $P$, $Q$
    with $P$ satisfying the hypothesis of this case, there are at most $$2^{(m-(4k+1)\log m)n}$$ matrices $X$ such that
    $PXQ=X$, by Lemma 1.3.13. The number of possibilities for $P$, $Q$ cannot exceed $n!m!$. Thus the number of
    possibilities for $X$ in the present case is at most $$2^{(m-(4k+1)\log m)n}n!m!$$ Thus also the number of
    $\mathcal{D}$-classes containing at least one such $X$ is at most $$2^{(m-(4k+1)\log m)n}n!m!$$ The ratio of this
    number to $${2^{nm} \over n!m!}$$ will approach zero.</p>
    <p>Case 2. $\mathcal{D}$-classes containing some matrix $X$ such that $PXQ=X$ for some $P$, $Q$ such that $Q$ has
    no more than $n-(4k+1)\log n$ cycles. This case is treated like Case 1.</p>
    <p>Case 3. $\mathcal{D}$-classes containing a matrix $X$ such that $PXQ=X$ for some $P$, $Q$ not both identity but
    such that $PXQ=X$ does not hold for any $P$, $Q$ with $P$ having no more that $m-(4k+1)\log m$ cycles or $Q$
    having no more than $n-(4k+1)\log n$ cycles. FOr such an $X$, choose a pair $P$, $Q$ satisfying $PXQ=X$ such that
    $\sup \{m-$number of cycles in $P, n-$number of cycles in $Q\}$ is a maximum. Let $s$ denote this maximum. We have
    $0 < s < (4k+1)(\sup\{\log m, \log n\})$. For a given $X$ the set $\{P: PXQ=X $for some $Q\}$ forms a group. Each
    elemnt of this group will fix at least $m-2s$ letters by Lemma 1.3.14. Thus by Lemma 1.3.12 the whole group will
    fix at least $m-4s$ letters. There is a similar group of $Q$'s which fixes at least $n-4s$ letters.</p>
    <p>FIx $s$. We first choose a set of $4s$ letters which is to contain the set of all non-fixed letter under $\{P:
    PXQ=X$ for some $Q\}$. There are $\begin{pmatrix}m \\ 4s \end{pmatrix}$ such choices. There is $\begin{pmatrix}n \\
    4s\end{pmatrix}$ choices for a similar set for $\{Q: PXQ=X$ for some $P\}$. Given that these sets are chosen, we
    can choose $P$ in $(4s)!$ ways to act on its set and $Q$ in $(4s)!$ ways to act on its set. Once $P$, $Q$ are
    chosen we can choose $X$ in at most $$2^{nm-s(\min\{n,m\})}$$ ways by Lemma 1.3.13. Thus for given $s$, there are at
    most $$\begin{pmatrix}m \\ 4s\end{pmatrix} \begin{pmatrix}m \\ 4s\end{pmatrix} (4s)! (4s)! 2^{nm - s (\min\{n,
    m\})}$$ choices of $X$ having the required value of $s$. However these $X$'s do not all lie in different
    $\mathcal{D}$-classes. For any permutation matrices $E$, $F$; $EXF$ lie in the $\mathcal{D}$-class and have the
    same value of $s$.</p>
    <p>How many different matrixes $EXF$ are there for a given $X$? We have a group action of the product of two
    symmetric groups on such matrices, sending $Y$ to $EYF^{-1}$. Here $F^{-1}$ denotes the inverse of $F$. The
    isotropy group of $X$ has order at most $(4s)!(4s)!$ by the remarks above about choosing $P$, $Q$ such that $PXQ =
    X$. Thus $\mathcal{D}$-class containing one $X$ also contains at least $${n!m! \over (4s)! (4s)!}$$ other matrices
    with the same $s$. Thus the number of $D$-classes containing matrices of this type for given $s$ is at most
    $${m^{4s} n^{4s} 2^{nm - s (min\{n, m\})} (4s)! (4s)! \over n!m!}$$ Allowing any value of $s$ we have at most
    $$1 \le s \le (4k+1) n_1^{m^{4s} n^{4s} 2^{nm-sn} 2 ((4s)!(4s)!)(4k+1)\log n_1 \over n!m!}$$ where $n_1 = \max \{n,
    m\}$ and $n_2 = \min \{n, m \}$. The ratio of this quantity to $${2^{nm} \over n!m!}$$ tends to zero.</p>
    <p>Case 4. All $PXQ$ are distinct so the $\mathcal{D}$-classes have at least $n!m!$ elements. There are at most
    $${2^{nm} \over n!m!}$$ $\mathcal{D}$-classes of this type. This proves the theorem. &#9647;</p>
    <p><b>Corollary 1.3.17.</b> Let $k$ be the number of matrices $X$ such that $PXQ=X$ for some $P$, $Q$ not both the
    identity. Then if $n, m \rightarrow \infty$ in such a way that $n \over m$ approaches a nonzero constant, $${k
    \over 2^{nm}}$$ approaches $0$.</p>
    <p><b>Theorem 1.3.18.</b> Under the hypotheses of Lemma 1.3.12, then the numbers of $\mathcal{D}$-classes of
    $\mathcal{D}$-classes of $B_{mn}$ are asymptotically equal to $${2^{nm} \over n!}, {2^{nm} \over m!}$$ respectively.
    The number of $\mathcal{H}$-classes is asymptotically equal to $2^{nm}$.</p>
    <p><b>Proof.</b> FOr an upper bound, for instance for $\mathcal{R}$-classes we have $$\begin{pmatrix}2^m \\
    n\end{pmatrix} + \begin{pmatrix}2^m \\ n-1\end{pmatrix} + ... + \begin{pmatrix}2^m \\ 1\end{pmatrix}$$ for column
    rank $k$, by choosing a set of $k$ column vectors to be a column basis. This is less than or equal to
    $$\begin{pmatrix}2^m \\ n\end{pmatrix} \sum_{i=1}^\infty ({n \over 2^m -1})^i$$ which gives the theorem. Similar
    methods apply in the other cases. This proves the theorem. &#9647;</p>
{% endblock %}
