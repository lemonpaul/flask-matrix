{% extends "base.html" %}

{% block app_content %}
    <h1>Fundamental concepts</h1>
    <p class="indent">A <i>Boolean algebra</i> is a mathematical system $(\beta, +, \cdot)$ consisting of a nonempty
    set $\beta$ and two binary operations $+$ and $\cdot$ defined on $\beta$ such that (1) seach of the operations $+$
    and $\cdot$ is commutative; (2) each operation is distibutive over the other; (3) there exist distinct identity
    element $0$ and $1$ relative to operations $+$ and $\cdot$ respectively, that is, $a+0=a$, $a*1=a$ for all
    $a\in\beta$; (4) for each element $a\in\beta$, there exists an element $a^C\in\beta$, called <i>complement</i> of
    $a$, such that $a+a^C=1$, $a\cdot a^C=0$.</p>
    <p class="indent">We will primarily work with the two element Boolean algebra. We shall use $\beta_0$ to denote the
    set $\{0,1\}$ with three operations $+$, $\cdot$, $^C$ defined as follows: $0+0=0\cdot 1=1 \cdot 0 = 0\cdot 0=0$,
    $1+ 0=0+1=1+1=1\cdot 1=1$, $0^C=1$, and $1^C=0$.</p>
    <h3>Boolean vectors</h3>
    <p class="indent"><b>Definition 1.1.</b> Let $V_n$ denote the set of all $n$-tuples $(a_1, a_2, ..., a_n)$ over
    $\beta_0$. An element of $V_n$ is called a <i>Boolean vector</i> of dimension $n$. The system $V_n$ together with
    the operation of component-wise addition is called the <i>Boolean vector space of dimension $n$</i>.</p>
    <p class="indent"><b>Definition 1.2.</b> Let $V^n=\{v^T: v \in V_n\}$, where $v^T$ we mean <i>column vector</i></p>
    <p class="center">$\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$</p>
    <p class="indent"><b>Definition 1.3.</b> Define the <i>complement</i> $v^C$ of $v$ to be the vector such that
    $v_i^C=1$ if and only if $v_i=0$, where $v_i$ denote the $i$the component of $v$.</p>
    <p class="indent"><b>Definition 1.4.</b> Let $e_i$ be the $n$-tuple with $1$ as $i$th coordinate, $0$ otherwise.
    Further, we define $e^i=e_i^T$.</p>
    <p class="indent"><b>Definition 1.5.</b> A $subspace$ of $V_n$ is a subset containing the zero vector and closed
    under addition of vectors. The $span$ of a set $W$ of vectors, denoted $\lt W\gt$, is the intersection of all
    subspaces containing $W$.</p>
    <p class="indent"><b>Definition 1.6.</b> Let $W\subset V_n$. A vector $v\in V_n$ is said to be <i>dependent</i> on
    $W$ if and only if $v\in\lt W\gt$. A set $W$ is said to be <i>independent</i> if ond only if for all $v\in W$, $v$
    is not dependent on $W\setminus\{v\}$, where $\setminus$ denotes the set-theoretic difference. If $W$ is not
    independent we say that it is dependent.</p>
    <p class="indent"><b>Definition 1.7.</b> Let $u, v \in V_n$, the we say that $u\le v$ if and only if $v_i=1$ for
    each $i$ such that $u_i=1$. We say that $u\lt v$ if $u \le v$ and $u \ne v$.</p>
    <p class="indent"><b>Definition 1.8.</b> Let $W \subset V_n$. A subset $B$ of $W$ is called a <i>basis</i> of $W$
    if and only if $W = \lt B \gt$, and $B$ is an independent set.</p>
    <p class="indent"><b>Theorem 1.1</b> Let $W$ be a subspace of $V_n$. Then there exists one subset $B$ of $V_n$ such
    that $B$ is a basis of $W$.</p>
    <p class="indent"><b>Proof.</b> Let $B$ be the set of all vectors of $W$ that are not sums of vectors of $W$
    smaller then themselves. Then $B$ is an independent set. Suppose $B$ generates a proper subspace of $W$. The let
    $v$ be a minimal vector of $W \setminus \lt B \gt$. Then $v$ is expressible as a sum of smaller vectors since it is
    not in $B$. But these smaller vectors must be in $\lt B \gt$ since $v$ was minimal. Thus $v$ belongs in the
    subspace generated by $B$. This is a contradiction. Thus $B$ generates $W$ and is a basis. By independence, $B$
    must be contained in every basis. Let $B'$ be another basis and let $u$ be a minimal element of $B'\setminus B$.
    Then by the reasoning above $u$ is dependent. This contradiction shows $B'=B$. This proves the theorem. &#9647;</p>
    <h3>Boolean Matrices</h3>
    <p class="indent"><b>Definition 2.1.</b> By a <i>Boolean matrix</i> of size $m \times n$ is meant an $m \times n$
    matrix over $\beta_0$. Let $B_{mn}$ denote the set of all $m \times n$ such matrices. If $m=n$, we write $B_n$.
    Elements of $B_{mn}$ are often called <i>relation matrices</i>, <i>Boolean relation matrices</i>, <i>binary
    relation matrices</i>,
    <i>binary Boolean matrices</i>, <i>(0, 1)-Boolean matrices</i>, and <i>(0,1)-matrices</i>.</p>
    <p class="indent"><b>Definition 2.2.</b> Let $A = (a_{ij}) \in B_{mn}$. Then the element $a_{ij}$ is called the
    <i>$(i,j)$-entry</i> of $A$. The $(i,j)$-entry of $A$ is sometimes designated by $A_{ij}$. The $i$th <i>row</i> of
    $A$ is the sequence $a_{i1}, a_{i2}, ..., a_{in}$, and $j$th <i>column</i> of $A$ is the sequence $a_{1j}, a_{2j},
    ... a_{mj}$. Let $A_{i*}$ ($A_{*i}$) denote the $i$th row (column) of $A$.</p>
    <p class="indent"><b>Definition 2.3.</b> The $n \times m$ <i>zero matrix $0$</i> is the matrix all of whose entries
    are zero. The $n \times n$ <i>identity matrix $I$</i> is the matrix $(\delta_{ij})$ such then $\delta_{ij}=1$ if
    $i=j$ and $\delta_{ij}=0$ if $i \ne j$. The $n \times m$ <i>universal matrix $J$</i> is the matrix all of whose
    entries are $1$.</p>
    <p class="indent"><b>Definition 2.4.</b> We use the notation $A^2$ to designate the product $AA$, $A^3=AA^2$, and
    in general $A^k=AA^{k-1}$ for any positive integer $k$. The matrix $A^k$ is called the $k$th <i>power</i> for
    obvious reasons. The motations $a^{(k)}_{ij}$ and $A^{(k)}_{ij}$ means $(i,j)$-entry and $(i,j)$-block of $A^k$.
    The notation $(A_{ij})^k$ means the $k$th power of the $(i,j)$-block of $A$.</p>
    <p class="indent"><b>Definition 2.5.</b> A <i>binary relation</i> on a set $X$ is a subset of $X \times X$. The
    <i>composition</i> of two binary relations $\rho_1$, $\rho_2$ is the raltion $\gamma$ such that $(x,y) \in \gamma$
    if and only if for some $z$ both $(x,z) \in \rho_1$ and $(z,y) \in \rho_2$.</p>
    <p class="indent"><b>Definition 2.6.</b> The <i>adjacency matrix $A_G$</i> of a directed graph (digraph) $G$ is the
    matrix $A_G=(a_{ij})$ such that $a_{ij}=1$ if there is an arc from vertex $V_i$ to vertex $v_j$ and $a_{ij}=0$
    otherwise. Dually, a digraph 127$G$ determines and is determined by the Boolean matrix $A_G$.</p>
    <p class="indent"><b>Definition 2.7.</b> A square matrix is called a <i>permutation matrix</i> if every row and
    every column contains exactly one $1$. Let $P_n$ denote the set of all $n \times n$ such matrices.</p>
    <p class="indent"><b>Definition 2.8.</b> The <i>transpose</i> of a Boolean matrix is obtained by rewriting its rows
    and columns. The transpose of $A$ will be denoted by $A^T$.</p>
    <p class="indent"><b>Definition 2.9.</b> A matrix is said to be a <i>partial permutation matrix</i> if every row
    and every column of it contains at most one $1$.</p>
    <p class="indent"><b>Definition 2.10.</b> Let $A, B \in B_{mn}$. By $B \le A$ we mean if $b_{ij}=1$ then $a_{ij}=1$
    for every $i$ and $j$.</p>
    <p class="indent"><b>Definition 2.11.</b> The <i>row space</i> of a matrix $A$ is the span of the set of all rows
    of $A$. Likewise one has a <i>column space</i>. Let $R(A)$ ($C(A)$) denote the row (column) space of $A$.</p>
    <p class="indent"><b>Definition 2.12.</b> By a <i>partial order relation</i> on a set $X$ we mean a reflexive,
    antisymmetric and transitive relation on $X$. A set $X$ together with a specific partial order relation $P$ in $X$
    is called a partially ordered set (poset). A <i>linear order</i> (also called <i>total order</i>) is a partial
    order relation $P$ such that for all $x,y$ $(x,y) \in P$ or $(y,x) \in P$.</p>
    <p class="indent"><b>Definition 2.13.</b> A <i>lattice</i> is a partially ordered set in which every pair of
    elements has a least upper bound (join) and greatest lower bound (meet). The operations join and meet are denoted
    by $\lor$ and $\land$ respectively.</p>
    <p class="indent"><b>Definition 2.14.</b> A lattice is said to be <i>distributive</i> if and only if $A \lor (B
    \land C) = (A \lor B) \land (A \lor C)$ for all $A, B, C$. This is equivalent to the dual condition $A \land (B
    \lor C)=(A \land B) \lor (A \land C)$.</p>
    <p class="indent"><b>Proposition 2.1.</b> If $A \in B_{mn}$, then $R(A)$ ($C(A)$) is a subspace of $V_n$
    ($V^m$).</p>
    <p class="indent"><b>Proof.</b> This relation holds for matrices over any semiring, and the proof is the same as in
    the case of matrices over $\mathbb{R}$. &#9647;</p>
    <p class="indent"><b>Proposition 2.2.</b> Let $A \in B_{mk}$, $B \in B_{kn}$, then $R(AB) \subseteq R(B)$ and
    $C(AB) \subseteq C(A)$.</p>
    <p class="indent"><b>Proof.</b> The proof follows immediately from the fact that the rows of $AB$ are sums of the
    rows of $B$, etc. &#9647;</p>
    <p class="indent"><b>Theorem 2.3.</b> If $A \in B_{mn}$, then $\lvert C(A) \rvert = \lvert R(A) \rvert$.</p>
    <p class="indent"><b>Proof.</b> We shall construct a bijection between $C(A)$ and $R(A)$. Let $v \in C(A)$, then
    there exists a unique set $\underline{s} \subset \underline{m} \subseteq \underline{n}$ such that </p>
    <p class="center">$$v=\sum_\underline{s} e^i$$</p>
    <p>Let $\underline{s}' = \underline{m} \setminus \underline{s}$ (we will use $'$ to denote set complements in this
    proof). Consider the map $f: C(A) \rightarrow R(A)$ given by </p>
    <p class="center">$$f(v) = \sum_{\underline{s}'} A_{i*}$$ </p>
    <p>where $v \in C(A)$. Clearly $f$ is well-defined. We claim that the following statements are true.</p>
    <p class="indent">(1) $f$ is injective: suppose we have $v,w \in C(A)$, $v \neq w$, and </p>
    <p class="center">$$v = \sum_\underline{s} e^i $$ </p>
    <p>while</p>
    <p class="center">$$w = \sum_\underline{t} e^i$$ </p>
    <p>where $\underline{t} \subset \underline{m}$ and that $f(v) = f(w)$, i.e.,</p>
    <p class="center">$$\sum_{s'} A_{i*} = \sum_{t'} A_{i*}$$</p>
    <p>Since $v \neq w$, we may assume that there exist a $p \in \underline{t} \setminus \underline{s}$. But since
    $w \in C(A)$ there exists a $k \in \underline{n}$ such that $a_{pk}=1$ and $A_{*k} \leq w$. Since $p \in
    \underline{s}'$, we must have $(f(v))_k=1$, which implies that there exists a $q \in \underline{t}'$ such that
    $a_{qk}=1$. But since $A_{*k} \leq w$, this implies that $e^q \leq w$, which is impossible since $q \in t'$. Thus
    $f(v) \neq f(w)$.</p>
    <p class="indent">(2) Since there exists an injection from the row space into the column space, we are through as
    it follows that $f$ is surjective. &#9647;</p>
    <p class="indent"><b>Corollary 2.4.</b> Let $A$ and $f$ be as in Theorem 2.3, and let $v,w \in C(A)$. Then $v \leq
    w$ if and only if $f(v) \geq f(w)$.</p>
    <p class="indent"><b>Proof.</b> <b>Necessity.</b> Clear, in that $\underline{s} \subset \underline{t}$ if and only
    if $\underline{s}' \supset \underline{t}'$. <b>Sufficiency. </b>This follows from the same proof as that given for
    injectivity of $f$. Assume $w \nleq v$ but $f(w) \geq f(v)$, and follow exactly the same procedure. &#9647;</p>
    <p class="indent"><b>Proposition 2.5.</b> Let $A_1, A_2, ..., A_k \in B_n$ and let $B = A_1 A_2 ... A_k$. Then
    $\lvert C(B)\rvert = \lvert R(B) \rvert \leq \lvert R(A_i) \rvert = \lvert R(A_I) \rvert = \lvert C(A_i) \rvert$
    for all $i$.</p>
    <p class="indent"><b>Proof.</b> For any $M, N$ we have $\lvert R(MN) \rvert \leq \lvert R(N) \rvert$ and $\lvert
    R(MN) \rvert = \lvert C(MN) \rvert \leq \lvert C(M) \rvert = \lvert R(M) \rvert$, by Proposition 2.2. The present
    proposition follows from this by induction. &#9647;</p>
    <p class="indent"><b>Definition 2.15.</b> Let $A \in B_{mn}$. By $B_r(A)$ we mean the unique basis of $R(A)$, and
    we call it the <i>row basis</i> of $A$. Similarly, by $B_c(A)$ we mean the unique basis of $C(A)$, which we call
    the <i>column basis</i> of $A$. The cardinality of $B_r(A)$ ($B_c(A)$) is called the <i>row (column) rank</i> of
    $A$ and is denoted by $\rho_r(A)$ ($\rho_c(A)$).</p>
    <h3>Green's Relations</h3>
    <p class="indent"><b>Definition 3.1.</b> A <i>right (left) ideal</i> in a semigroup $S$ is a subset $X$ such that
    $XS \subseteq X$ ($SX \subseteq X$), and the <i>(two sided) ideal</i> of $S$ generated by $X$ is $SXS \cup XS \cup
    SX$. <i> Principal ideals</i>, <i>principal left</i> and <i>right ideals</i> are defined in a similar way.</p>
    <p class="indent"><b>Definition 3.2.</b> Two elements of a semigroup $S$ are said to be
    <i>$\mathcal{L}$-equivalent</i> if they generate the same pricipal left ideal of $S$.
    <i>$\mathcal{R}$-equivalence</i> is defined dually. The join of the equivalence relations $\mathcal{L}$ and
    $\mathcal{R}$ is denoted by $\mathcal{D}$ and their intersection by $\mathcal{H}$. Two elements are said to be
    <i>$\mathcal{J}$-equivalent</i> if they generate the same two-sided pricipal ideal. These five relations are known
    as <i>Green's relations</i>.</p>
    <p class="indent"><b>Definition 3.3.</b> The <i>weight</i> of a vector $v$, denoted by $w(v)$, is the number of
    nonzero elements of $v$. The weight of $v$ is sometimes called the rank of the vector $v$.</p>
    <p class="indent"><b>Lemma 3.1.</b> Two matrices $A, B$ are $\mathcal{L}$ ($\mathcal{R}$)-equivalent if and only if
    they have the same row (column) space.</p>
    <p class="indent"><b>Proof.</b> Suppose $XA=B$ and $YB=A$. Then $R(B) \subseteq R(A)$ and $R(A) \subseteq R(B)$ so
    $R(A) = R(B)$. Suppose $R(B) \subseteq R(A)$. Then by looking at each row of $B$ we can find an $X$ such that
    $XA=B$. Likewise we can find a $Y$ such that $YB=A$. &#9647;</p>
    <p class="indent"><b>Lemma 3.2.</b> Let $U$ be any subspace of $V_n$ and $f$ a homomorphism of commutative
    semigroups from $U$ into $V_n$ such that $f(0)=0$. Then there exists a matrix $A$ such that for all $v \in V_n$,
    $vA = f(v)$.</p>
    <p class="indent"><b>Proof.</b> Let $S(i)=\{v \in U; v_i = 1 \}$. Then define $A_{i*}$ to be $inf \{ f(v): v \in
    S(i)\}$. We will show $vA=f(v)$, for all $v \in U$. Suppose $(vA)_j=1$. Then for some $k$, $v_k=1$ and $a_{kj}=1$.
    Thus for all $w \in S(k)$, $(f(w))_j=1$. Since $v \in S(k)$, $(f(v))_j=1$. This proves that $vA \le f(v)$.</p>
    <p class="indent">Suppose that $(vA)_j=0$. Then for all $k$ such that $v_k=1$, we have $a_{kj}=0$. Thus for all $k$
    such that $v_k=1$ we have a vector $x(k) \in S(k)$ such that $f(x(k))_j=0$. But $(\sum x(k))_k$ is $1$ for each $k$
    such that $v_k=1$. Thus $\sum x(k) \ge v$. Thus $\sum f(x(k)) = f(\sum x(k)) \ge f(v)$. Since $f(x(k))_j=0$ for
    each $k$, $f(v)_j = 0$. This proves $vA \ge f(v)$. &#9647;</p>
    <p class="indent"><b>Theorem 3.3.</b> Two matrices in $B_n$ belong in the same $\mathcal{D}$-class if and only if
    their row spaces are isomporphic as lattices.</p>
    <p class="indent"><b>Proof.</b> The row space of matrix is the same as its image space on row verctors. And two
    such spaces are isomorphic as lattices if and only if they are isomorphic as commutative semigroups.</p>
    <p class="indent">Suppose $A \mathcal{D} B$. Let $C$ be such that $A \mathcal{L} C$ and $C \mathcal{R} B$. Then
    $A$, $C$ have identical image spaces on row vectors. Let $X$, $Y$ be such that $CX=B$ and $BY=C$. Then we have maps
    $f: V_n C \rightarrow V_n B$ and $g: V_n B \rightarrow V_n C$ given by multiplying on the right by $X$ and $Y$. We
    have $fg$ and $gf$ are the identity. Thus the image spaces of $B$ adn $C$ are isomorphic. Thus if $A \mathcal{D}
    B$, $R(A) \simeq R(B)$.</p>
    <p class="indent">Suppose $R(A) \simeq R(B)$. Let $h$ be an isomorphism from $V_n A$ to $V_n B$. By Lemma 3.2 we
    have matrices matrices $X$, $Y$ such that $vX=h(v)$ for $v \in V_n A$ and $vY = h^{-1}(v)$ for $v \in V_n B$. Thus
    $XY$ is the identity on $V_n A$ and $YX$ is the identity on $V_n B$. Then $A=AXY$ and $V_nAX=V_n B$. Thus $A
    \mathcal{R} AX$ and $AX \mathcal{L} B$. So $A \mathcal{D} B$. This proves the theorem. &#9647;</p>
    <p class="indent"><b>Definition 3.4.</b> Let $S$ be a semigroup, and let $a \in S$. We define: $L_a=\{b \in S: a
    \mathcal{L} b\}$, $R_a = \{b \in S: a \mathcal{R} b\}$, $H_a = \{b \in S: a \mathcal{H} b\}$, $D_a = \{b \in S: a
    \mathcal{D} b\}$, $J_a = \{b \in S: a \mathcal{J} b\}$.</p>
    <p class="indent"><b>Theorem 3.4.</b> Let $A \in B_n$. Then the elements of $H_a$ are in one-to-one correspondence
    with the lattice automorphisms of $R(A)$.</p>
    <p class="indent"><b>Proof.</b> Let $\alpha$ be an automorphism from $R(A)$ to $R(A)$. Then $A \alpha$ gives a
    linear map from $V_n$ to $V_n$ sending $0$ to $0$, i.e., a matrix $B$. The matrices $A$, $B$, both have image space
    $R(A)$, so they are $\mathcal{L}$-equivalent. By Lemman 3.2, there exist matrices $X$, $Y$ so that on $R(A)$ the
    equations $X=\alpha$ and $Y=\alpha^{-1}$ hold. Then for any vector $v$ it is true that $v A X = v A \alpha = vB$
    and $v B Y = v B \alpha^{-1} = v A \alpha \alpha^{-1} = v A$. So $AX=B$ and $BY=A$. Thus $A \mathcal{H} B$. This
    defines a function from the automorphisms of $R(A)$ into the $\mathcal{H}$-class of $A$. Two different
    automorphisms will give rise to different maps $A \alpha$ and so to different matrices $B$. Thus the function is
    one-to-one. Let $B$ be any matrix in the $\mathcal{H}$-class of $A$. Then $R(A)=R(B)$ and there exist matrices $X$
    and $Y$ such that $AX=B$ and $BY=A$. This implices that $X$ and $Y$ map $R(A)$ to itself and that $X$ gives an
    automorphism of $R(A)$. This proves the function is onto, and completes the proof of the theorem. &#9647;</p>
    <p class="indent"><b>Lemma 3.5.</b> The number of permutations of $n$ objects with repetitions allowed wich may be
    formed from $p$ objects of which $k$ have been singled out to appear in every one of these permutations is </p>
    <p class="center">$$\sum_{i=0}^k (-1)^i \begin{pmatrix}k \\ i \end{pmatrix} (p-i)^n$$</p>
    <p class="indent"><b>Proof.</b> We will prove the lemma by using generating functions for permutations although the
    lemma can be proved directly by analyzing which kinds of permutations are permissible under the rule stated in the
    lemma.</p>
    <p class="indent">The generating function for the situation as described in the above will be </p>
    <p class="center">$$(1+ t + {t^2 \over 2!} + ...)^{p-k} = (e^t-1)^k (e^t)^{p-k} \sum_{i=0}^k (-1)^i
    \begin{pmatrix} k \\ i \end{pmatrix} e^{(k-i) t} e^{(p-k) t} = \sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i
    \end{pmatrix} e^{(p-i) t}$$</p>
    <p>Thus this equation can be written as</p>
    <p class="center">$$\sum_{n=0}^\infty \sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} (p-i)^n
    {t^n \over n!}$$</p>
    <p>Once we have the generating function, the answer to our problem is the coefficient of $t^n/n!$, which turns out
    to be </p>
    <p class="center">$$\sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} (p-i)^n$$</p>
    <p>This completes the proof. &#9647;</p>
    <p class="indent"><b>Corollary 3.6.</b> For any integer $n$,</p>
    <p class="center">$$n! = \sum_{i=0}^n (-1)^i \begin{pmatrix} n \\ i \end{pmatrix} (n-i)^n.$$</p>
    <p class="indent"><b>Proof.</b> The right hand side of the equality written above is the number of permutations of
    $n$ objects of which $n$ have been singled out to appear of all permutations of $n$ objects without repetitions.
    &#9647;</p>
    <p class="indent"><b>Theorem 3.7.</b> If $A \in B_n$, $\rho_r(A)=s$, $\rho_c(A)=t$, $\lvert H_A \rvert = h$, and
    $\lvert R(A) \rvert = \lvert C(A) \rvert \rvert = p$, then </p>
    <p class="indent">(1) $\vert L_A \rvert = \sum_{i=0}^s (-1)^i \begin{pmatrix} s \\ i \end{pmatrix} (p-i)^n$,
    $\lvert R_A \rvert = \sum_{i=0}^t (-1)^i \begin{pmatrix} t \\ i \end{pmatrix} (p-i)^n$</p>
    <p>(2) the number of $\mathcal{L}$-classes in $D_A$ is $h^{-1} \lvert R_A \rvert$ and similarly the number of
    $\mathcal{R}$-classes in $D_A$ is $h^{-1} \lvert L_A \rvert$. The number of $\mathcal{H}$-classes equals the number
    of $\mathcal{L}$-classes multiplied by the number of $\mathcal{R}$-classes or $h^{-2} \lvert L_A \rvert \lvert R_A
    \rvert$</p>
    <p>(3) $\lvert D_A \rvert = h$ (number of $\mathcal{L}$-classes) (number of $\mathcal{R}$-classes $= h^{-1} \lvert
    L_A \rvert \lvert R_A \rvert$</p>
    <p class="indent"><b>Proof.</b> Follows directly from structure of Green's equivalence class and Lemma 3.5.
    &#9647;</p>
    <p class="indent"><b>Corollary 3.8.</b> Let $A \in B_n$ and $\rho_r(A) = \rho_c(A)$. Then $\lvert L_A \rvert =
    \lvert R_A \rvert$ and the number of $\mathcal{L}$-classes in $D_A$ equals the number of $\mathcal{R}$-classes in
    $D_A$.</p>
    <p class="indent"><b>Theorem 3.9.</b> For a matrix $A$ let $A_0$ be the matrix obtained from $A$ by replacing all
    dependent rows and columns of $A$, and all but one copy of each independent row and column of $A$, by zero. Then
    $A \mathcal{D} A_0$ so $H_A = \lvert H_{A_0} \rvert$. And $H_{A_0} = \{P A_0: P A_0 = A_0 Q$ for some permutation
    matrices $P, Q\}$.</p>
    <p class="indent"><b>Proof.</b> There is a natural isomorphism between the row sapce of $A$ and that of $A_0$, so $A$
    and $A_0$ are $\mathcal{D}$-equivalent. Thus $\lvert H_A \rvert = \lvert H_{A_0} \rvert$. The indicated set of
    matrices are both $\mathcal{R}$ and $\mathcal{L}$-equivalent to $A_0$ so they lie in $H_{A_0}$.</p>
    <p class="indent">Conversely let $X \in H_{A_0}$. Then $X$ has the same row space and the same column space as
    $A_0$. Suppose $X_{i*} \neq 0$ but $(A_0)_{i*}=0$. Then some column of $X$ has its $i$-entry nonzero. But no column
    of $A$ has $i$-entry nonzero. This is a contradiction.</p>
    <p class="indent">Thus $X$ can have no more nonzero rows than $A_0$. But $X$ must contain all the nonzero rows of
    $A_0$, since the nonzero rows are distinct basis vectors. Thus the rows of $X$ are permutations of the rows of
    $A_0$. Thus $X = P A_0$ for some permutation matrix $P$. Likewise $X = A_0 Q$ for some permutation matrix $Q$. This
    proves the theorem. &#9647;</p>
{% endblock %}

{% block scripts %}
    <script type="text/javascript" id="MathJax-script" async
            src="http://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$']]
          }
        };
    </script>
{% endblock %}