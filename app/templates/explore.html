{% extends "base.html" %}

{% block app_content %}
    <h1>Fundamental concepts</h1>
    <p>A <i>Boolean algebra</i> is a mathematical system $(\beta, +, \cdot)$ consisting of a nonempty set $\beta$ and two
    binary operations $+$ and $\cdot$ defined on $\beta$ such that (1) seach of the operations $+$ and $\cdot$ is
    commutative; (2) each operation is distibutive over the other; (3) there exist distinct identity element $0$ and $1$
    relative to operations $+$ and $\cdot$ respectively, that is, $a+0=a$, $a*1=a$ for all $a\in\beta$; (4) for each
    element $a\in\beta$, there exists an element $a^C\in\beta$, called <i>complement</i> of $a$, such that $a+a^C=1$,
    $a\cdot a^C=0$.</p>
    <p>We will primarily work with the two element Boolean algebra. We shall use $\beta_0$ to denote the
    set $\{0,1\}$ with three operations $+$, $\cdot$, $^C$ defined as follows: $0+0=0\cdot 1=1 \cdot 0 = 0\cdot 0=0$, $1+
    0=0+1=1+1=1\cdot 1=1$, $0^C=1$, and $1^C=0$.</p>
    <h3>Boolean vectors</h3>
    <p><b>Definition 1.1.</b> Let $V_n$ denote the set of all $n$-tuples $(a_1, a_2, ..., a_n)$ over $\beta_0$. An element of
    $V_n$ is called a <i>Boolean vector</i> of dimension $n$. The system $V_n$ together with the opration of component-wise
    addition is called the <i>Boolean vector space of dimension n</i>.</p>
    <p><b>Definition 1.2.</b> Let $V^n=\{v^T: v \in V_n\}$, where $v^T$ we mean <i>column vector</i></p>
    <p style="text-align: center;">$\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$</p>
    <p><b>Definition 1.3.</b> Define the <i>complement</i> $v^C$ of $v$ to be the vector such thet $v_i^C=1$ if and only if
    $v_i=0$, where $v_i$ denote the $i$the component of $v$.</p>
    <p><b>Definition 1.4.</b> Let $e_i$ be the $n$-tuple with $1$ as $i$th coordinate, $0$ otherwise. Further, we define
    $e^i=e_i^T$.</p>
    <p><b>Definition 1.5.</b> A $subspace$ of $V_n$ is a subset containing the zero vector and closed under addition of
    vectors. The $span$ of a set $W$ of vectors, denoted $\lt W\gt$, is the intersection of all subspaces containing $W$.</p>
    <p><b>Definition 1.6.</b> Let $W\subset V_n$. A vector $v\in V_n$ is said to be <i>dependent</i> on $W$ if and only if
    $v\in\lt W\gt$. A set $W$ is said to be <i>independent</i> if ond only if for all $v\in W$, $v$ is not dependent on
    $W\setminus\{v\}$, where $\setminus$ denotes the set-theoretic difference. If $W$ is not independent we say that it
    is dependent.</p>
    <p><b>Definition 1.7.</b> Let $u, v \in V_n$, the we say that $u\le v$ if and only if $v_i=1$ for each $i$ such that
    $u_i=1$. We say that $u\lt v$ if $u \le v$ and $u \ne v$.</p>
    <p><b>Definition 1.8.</b> Let $W \subset V_n$. A subset $B$ of $W$ is called a <i>basis</i> of $W$ if and only if
    $W = \lt B \gt$, and $B$ is an independent set.</p>
    <p><b>Theorem 1.1</b> Let $W$ be a subspace of $V_n$. Then there exists one subset $B$ of $V_n$ such that $B$ is a
    basis of $W$.</p>
    <p><b>Proof.</b> Let $B$ be the set of all vectors of $W$ that are not sums of vectors of $W$ smaller then themselves.
    Then $B$ is an independent set. Suppose $B$ generates a proper subspace of $W$. The let $v$ be a minimal vector of
    $W \setminus \lt B \gt$. Then $v$ is expressible as a sum of smaller vectors since it is not in $B$. But these smaller
    vectors must be in $\lt B \gt$ since $v$ was minimal. Thus $v$ belongs in the subspace generated by $B$. This is a
    contradiction. Thus $B$ generates $W$ and is a basis. By independence, $B$ must be contained in every basis. Let $B'$
    be another basis and let $u$ be a minimal element of $B'\setminus B$. Then by the reasoning above $u$ is dependent.
    This contradiction shows $B'=B$. This proves the theorem. &#9647;</p>
    <h3>Boolean Matrices</h3>
    <p><b>Definition 2.1.</b> By a <i>Boolean matrix</i> of size $m \times n$ is meant an $m \times n$ matrix over
    $\beta_0$. Let $B_{mn}$ denote the set of all $m \times n$ such matrices. If $m=n$, we write $B_n$. Elements of
    $B_{mn}$ are often called <i>relation matrices</i>, <i>Boolean relation matrices</i>, <i>binary relation matrices</i>,
    <i>binary Boolean matrices</i>, <i>(0, 1)-Boolean matrices</i>, and <i>(0,1)-matrices</i>.</p>
    <p><b>Definition 2.2.</b> Let $A = (a_{ij}) \in B_{mn}$. Then the element $a_{ij}$ is called the <i>$(i,j)$-entry</i>
    of $A$. The $(i,j)$-entry of $A$ is sometimes designated by $A_{ij}$. The $i$th <i>row</i> of $A$ is the sequence
    $a_{i1}, a_{i2}, ..., a_{in}$, and $j$th <i>column</i> of $A$ is the sequence $a_{1j}, a_{2j}, ... a_{mj}$. Let
    $A_{i*}$ ($A_{*i}$) denote the $i$th row (column) of $A$.</p>
    <p><b>Definition 2.3.</b> The $n \times m$ <i>zero matrix $0$</i> is the matrix all of whose entries are zero. The
    $n \times n$ <i>identity matrix $I$</i> is the matrix $(\delta_{ij})$ such then $\delta_{ij}=1$ if $i=j$ and
    $\delta_{ij}=0$ if $i \ne j$. The $n \times m$ <i>universal matrix $J$</i> is the matrix all of whose entries are $1$.</p>
    <p><b>Definition 2.4.</b> We use the notation $A^2$ to designate the product $AA$, $A^3=AA^2$, and in general
    $A^k=AA^{k-1}$ for any positive integer $k$. The matrix $A^k$ is called the $k$th <i>power</i> for obvious reasons.
    The motations $a^{(k)}_{ij}$ and $A^{(k)}_{ij}$ means $(i,j)$-entry and $(i,j)$-block of $A^k$. The notation $(A_{ij})^k$
    means the $k$th power of the $(i,j)$-block of $A$.</p>
    <p><b>Definition 2.5.</b> A <i>binary relation</i> on a set $X$ is a subset of $X \times X$. The <i>composition</i>
    of two binary relations $\rho_1$, $\rho_2$ is the raltion $\gamma$ such that $(x,y) \in \gamma$ if and only if for
    some $z$ both $(x,z) \in \rho_1$ and $(z,y) \in \rho_2$.</p>
    <p><b>Definition 2.6.</b> The <i>adjacency matrix $A_G$</i> of a directed graph (digraph) $G$ is the matrix $A_G=(a_{ij})$
    such that $a_{ij}=1$ if there is an arc from vertex $V_i$ to vertex $v_j$ and $a_{ij}=0$ otherwise. Dually, a digraph
    $G$ determines and is determined by the Boolean matrix $A_G$.</p>
    <p><b>Definition 2.7.</b> A square matrix is called a <i>permutation matrix</i> if every row and every column contains
    exactly one $1$. Let $P_n$ denote the set of all $n \times n$ such matrices.</p>
    <p><b>Definition 2.8.</b> The <i>transpose</i> of a Boolean matrix is obtained by rewriting its rows and columns. The
    transpose of $A$ will be denoted by $A^T$.</p>
    <p><b>Definition 2.9.</b> A matrix is said to be a <i>partial permutation matrix</i> if every row and every column of
    it contains at most one $1$.</p>
    <p><b>Definition 2.10.</b> Let $A, B \in B_{mn}$. By $B \le A$ we mean if $b_{ij}=1$ then $a_{ij}=1$ for every $i$ and
    $j$.</p>
    <p><b>Definition 2.11.</b> The <i>row space</i> of a matrix $A$ is the span of the set of all rows of $A$. Likewise one has a <i>column space</i>.
    Let $R(A)$ ($C(A)$) denote the row (column) space of $A$.</p>
    <p><b>Definition 2.12.</b> By a <i>partial order relation</i> on a set $X$ we mean a reflexive, antisymmetric and
    transitive relation on $X$. A set $X$ together with a specific partial order relation $P$ in $X$ is called a partially
    ordered set (poset). A <i>linear order</i> (also called <i>total order</i>) is a partial order relation $P$ such that
    for all $x,y$ $(x,y) \in P$ or $(y,x) \in P$.</p>
    <p><b>Definition 2.13.</b> A <i>lattice</i> is a partially ordered set in which every pair of elements has a least
    upper bound (join) and greatest lower bound (meet). The operations join and meet are denoted by $\lor$ and $\land$
    respectively.</p>
    <p><b>Definition 2.14.</b> A lattice is said to be <i>distributive</i> if and only if $A \lor (B \land C) = (A \lor B)
    \land (A \lor C)$ for all $A, B, C$. This is equivalent to the dual condition $A \land (B \lor C)=(A \land B) \lor
    (A \land C)$.</p>
    <p><b>Proposition 2.1.</b> If $A \in B_{mn}$, then $R(A)$ ($C(A)$) is a subspace of $V_n$ ($V^m$).</p>
    <p><b>Proof.</b> This relation holds for matrices over any semiring, and the proof is the same as in the case of
    matrices over $\mathbb{R}$. &#9647;</p>
    <p><b>Proposition 2.2.</b> Let $A \in B_{mk}$, $B \in B_{kn}$, then $R(AB) \subseteq R(B)$ and $C(AB) \subseteq
    C(A)$.</p>
    <p><b>Proof.</b> The proof follows immediately from the fact that the rows of $AB$ are sums of the rows of $B$, etc.
    &#9647;</p>
    <p><b>Theorem 2.3.</b> If $A \in B_{mn}$, then $\lvert C(A) \rvert = \lvert R(A) \rvert$.</p>
    <p><b>Proof.</b> We shall construct a bijection between $C(A)$ and $R(A)$. Let $v in C(A)$, then there exists a unique
    set $\underline{s} \subset \underline{m} \subseteq \underline{n}$ such that </p>
    <p style="text-align: center;">$$v=\sum_\underline{s} e^i$$</p>
    <p>Let $\underline{s}' = \underline{m} \setminus \underline{s}$ (we will use $'$ to denote set complements in this
    proof). Consider the map $f: C(A) \rightarrow R(A)$ given by </p>
    <p style="text-align: center;">$$f(v) = \sum_{\underline{s}'} A_{i*}$$ </p>
    <p>where $v \in C(A)$. Clearly $f$ is well-defined. We claim that the following statements are true.</p>
    <p>(1) $f$ is injective: suppose we have $v,w \in C(A)$, $v \neq w$, and </p>
    <p style="text-align: center;">$$v = \sum_\underline{s} e^i $$ </p>
    <p>while</p>
    <p style="text-align: center;">$$w = \sum_\underline{t} e^i$$ </p>
    <p>where $\underline{t} \subset \underline{m}$ and thet $f(v) = f(w)$, i.e.,</p>
    <p style="text-align: center;">$$\sum_{s'} A_{i*} = \sum_{t'} A_{i*}$$</p>
    <p>Since $v \neq w$, we may assume thet there exist a $p \in \underline{t} \setminus \underline{s}$. But since
    $w \in C(A)$ there exists a $k \in \underline{n}$ such that $a_{pk}=1$ and $A_{*k} \leq w$. SInce $p \in
    \underline{s}'$, we must have $(f(v))_k=1$, which implies that there exists a $q \in \underline{t}'$ such that
    $a_{qk}=1$. But since $A_{*k} \leq w$, this implies that $e^q \leq w$, which is impossible since $q \in t'$. Thus
    $f(v) \neq f(w)$.</p>
    <p>Since there exists an injection from the row space into the column space, we are through as it follows that $f$
    is surjective. &#9647;</p>
    <p><b>Corollary 2.4.</b> Let $A$ and $f$ be as in Theorem 2.3, and let $v,w \in C(A)$. Then $v \leq w$ if and only if
    $f(v) \geq f(w)$.</p>
    <p><b>Proof.</b> <b>Necessity.</b> Clear, in that $\underline{s} \subset \underline{t}$ if and only if $\underline{s}'
    \supset \underline{t}'$. <b>Sufficiency. </b>This follows from the same proof as that given for injectivity of $f$.
    Assume $w \nleq v$ but $f(w) \geq f(v)$, and follow exactly the same procedure. &#9647;</p>
    <p><b>Proposition 2.5.</b> Let $A_1, A_2, ..., A_k \in B_n$ and let $B = A_1 A_2 ... A_k$. Then $\lvert C(B)\rvert =
    \lvert R(B) \rvert \leq \lvert R(A_i) \rvert = \lvert R(A_I) \rvert = \lvert C(A_i) \rvert$ for all $i$.</p>
    <p><b>Proof.</b> For any $M, N$ we have $\lvert R(MN) \rvert \leq \lvert R(N) \rvert$ and $\lvert R(MN) \rvert =
    \lvert C(MN) \rvert \leq \lvert C(M) \rvert = \lvert R(M) \rvert$, by Proposition 2.2. The present proposition follows
    from this by induction. &#9647;</p>
    <p><b>Definition 2.15.</b> Let $A \in B_{mn}$. By $B_r(A)$ we mean the unique basis of $R(A)$, and we call it the
    <i>row basis</i> of $A$. Similarly, by $B_c(A)$ we mean the unique basis of $C(A)$, which we call the <i>column basis</i>
    of $A$. The cardinality of $B_r(A)$ ($B_c(A)$) is called the <i>row (column) rank</i> of $A$ and is denoted by
    $\rho_r(A)$ ($\rho_c(A)$).</p>
{% endblock %}

{% block scripts %}
    <script type="text/javascript" id="MathJax-script" async
            src="http://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$']]
          }
        };
    </script>
{% endblock %}